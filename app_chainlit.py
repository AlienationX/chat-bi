import threading
import time
from pathlib import Path

import chainlit as cl
from chainlit.data.sql_alchemy import SQLAlchemyDataLayer
from chainlit.data.storage_clients.base import BaseStorageClient
from chainlit.input_widget import Select, Slider, Switch, Tags
from chainlit.types import ThreadDict
from langchain.agents import create_agent
from langchain_ollama import ChatOllama
from langgraph.config import get_stream_writer
from mcp import ClientSession

# å¯¼å…¥å…¼å®¹OpenAIçš„å®¢æˆ·ç«¯åº“
from openai import AsyncOpenAI

# å…¨å±€å˜é‡å­˜å‚¨ MCP æœåŠ¡çº¿ç¨‹
_mcp_threads = []
_mcp_servers = []

"""chainlit run app.py --port 8000 --workers 2"""

# è®¾ç½® Chainlit JWT secretï¼ˆç”¨äºå¯†ç è®¤è¯ï¼‰
# å¦‚æœç¯å¢ƒå˜é‡ä¸­å·²è®¾ç½®ï¼Œåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„å€¼ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤å€¼

commands = [
    {"id": "Picture", "icon": "image", "description": "Use DALL-E"},
    {"id": "Search", "icon": "globe", "description": "Find on the web"},
    {
        "id": "Canvas",
        "icon": "pen-line",
        "description": "Collaborate on writing and code",
    },
    {"id": "Database", "icon": "database", "description": "Use database"},
]


# è‡ªå®šä¹‰æœ¬åœ°å­˜å‚¨å®¢æˆ·ç«¯
class LocalStorageClient(BaseStorageClient):
    """æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨å®¢æˆ·ç«¯"""

    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)

    async def upload_file(self, file_path: str, file_content: bytes) -> str:
        """ä¸Šä¼ æ–‡ä»¶åˆ°æœ¬åœ°å­˜å‚¨"""
        target_path = self.base_path / file_path
        target_path.parent.mkdir(parents=True, exist_ok=True)
        target_path.write_bytes(file_content)
        return str(target_path)

    async def get_read_url(self, file_path: str) -> str:
        """è·å–æ–‡ä»¶çš„è¯»å–URL"""
        return str(self.base_path / file_path)

    async def delete_file(self, file_path: str) -> None:
        """åˆ é™¤æ–‡ä»¶"""
        target_path = self.base_path / file_path
        if target_path.exists():
            target_path.unlink()

    async def close(self) -> None:
        """å…³é—­å­˜å‚¨å®¢æˆ·ç«¯"""
        pass


def get_weather(city: str) -> str:
    """Get weather for a given city."""
    print(">>> tool message: get_weather", city)
    writer = get_stream_writer()
    writer(f"Looking up data for city: {city}")
    writer(f"Acquired data for city: {city}")
    return f"It's always sunny in {city}!"


def _run_mcp_server(module_name: str, server_name: str, port: int, transport: str = "streamable-http"):
    """åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œ MCP æœåŠ¡å™¨"""
    try:
        # åŠ¨æ€å¯¼å…¥ MCP æ¨¡å—
        import importlib

        module_path = f"mcp_servers.{module_name}"

        try:
            module = importlib.import_module(module_path)
        except ImportError as import_error:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç›´æ¥å¯¼å…¥æ¨¡å—
            print(f"âš ï¸ å°è¯•å¯¼å…¥ {module_path} å¤±è´¥: {import_error}")
            # å°è¯•ä½¿ç”¨ sys.path æ·»åŠ å½“å‰ç›®å½•
            import sys
            from pathlib import Path

            current_dir = Path(__file__).parent
            if str(current_dir) not in sys.path:
                sys.path.insert(0, str(current_dir))
            module = importlib.import_module(f"mcp_servers.{module_name}")

        # è·å– mcp å¯¹è±¡
        mcp_instance = getattr(module, "mcp", None)
        if mcp_instance is None:
            print(f"âŒ æœªæ‰¾åˆ° {server_name} MCP æœåŠ¡å™¨å®ä¾‹ (æ¨¡å—: {module_path})")
            return

        # ç›´æ¥ä¿®æ”¹ç«¯å£è®¾ç½®
        if hasattr(mcp_instance, "settings"):
            mcp_instance.settings.port = port
            mcp_instance.settings.host = "127.0.0.1"
        else:
            print(f"âš ï¸ {server_name} MCP æœåŠ¡å™¨å®ä¾‹æ²¡æœ‰ settings å±æ€§ï¼Œä½¿ç”¨é»˜è®¤ç«¯å£")

        print(f"ğŸš€ å¯åŠ¨ {server_name} MCP æœåŠ¡å™¨ (transport: {transport}, port: {port})...")
        # è¿è¡Œ MCP æœåŠ¡å™¨ï¼ˆé˜»å¡è°ƒç”¨ï¼‰
        mcp_instance.run(transport=transport)
    except Exception as e:
        print(f"âŒ {server_name} MCP æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {str(e)}")
        import traceback

        traceback.print_exc()


def start_mcp():
    """å¯åŠ¨ MCP æœåŠ¡å™¨"""
    global _mcp_threads, _mcp_servers

    print("ğŸš€ æ­£åœ¨å¯åŠ¨ MCP æœåŠ¡å™¨...")

    # å®šä¹‰è¦å¯åŠ¨çš„ MCP æœåŠ¡ (æ¨¡å—å, æœåŠ¡å™¨å, ç«¯å£, ä¼ è¾“æ–¹å¼)
    # ä» 8001 å¼€å§‹åˆ†é…ç«¯å£ï¼Œé¿å…ä¸ Chainlit çš„ 8000 ç«¯å£å†²çª
    mcp_services = [
        ("db_postgresql", "PostgreSQL Database", 8001, "streamable-http"),
        ("math", "Math", 8002, "streamable-http"),
    ]

    # ä¸ºæ¯ä¸ªæœåŠ¡åˆ›å»ºåå°çº¿ç¨‹
    for module_name, server_name, port, transport in mcp_services:
        thread = threading.Thread(
            target=_run_mcp_server,
            args=(module_name, server_name, port, transport),
            daemon=True,  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹ï¼Œä¸»è¿›ç¨‹é€€å‡ºæ—¶è‡ªåŠ¨ç»ˆæ­¢
            name=f"MCP-{server_name}",
        )
        thread.start()
        _mcp_threads.append(thread)
        print(f"âœ… {server_name} MCP æœåŠ¡å™¨çº¿ç¨‹å·²å¯åŠ¨ (ç«¯å£: {port})")

    # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æœåŠ¡å™¨å¯åŠ¨
    time.sleep(1)
    print(f"âœ… å·²å¯åŠ¨ {len(_mcp_threads)} ä¸ª MCP æœåŠ¡å™¨")


def stop_mcp():
    """åœæ­¢ MCP æœåŠ¡å™¨"""
    global _mcp_threads, _mcp_servers

    print("ğŸ›‘ æ­£åœ¨åœæ­¢ MCP æœåŠ¡å™¨...")

    # ç”±äºä½¿ç”¨å®ˆæŠ¤çº¿ç¨‹ï¼Œä¸»è¿›ç¨‹é€€å‡ºæ—¶ä¼šè‡ªåŠ¨ç»ˆæ­¢
    # ä½†æˆ‘ä»¬å¯ä»¥æ˜¾å¼åœ°æ ‡è®°å®ƒä»¬
    for thread in _mcp_threads:
        if thread.is_alive():
            print(f"ğŸ›‘ åœæ­¢ MCP æœåŠ¡å™¨çº¿ç¨‹: {thread.name}")

    _mcp_threads.clear()
    _mcp_servers.clear()
    print("âœ… MCP æœåŠ¡å™¨å·²åœæ­¢")


@cl.step(type="tool")
async def tool():
    print("tool called...")
    await cl.sleep(1)
    return "Response from the tool!"


# è¿™ä¸ªå‡½æ•°åªåœ¨åº”ç”¨å¯åŠ¨æ—¶æ‰§è¡Œä¸€æ¬¡
@cl.on_app_startup
async def app_startup():
    start_mcp()
    print("âœ… åº”ç”¨å¯åŠ¨ï¼šAIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–")


@cl.on_app_shutdown
async def app_shutdown():
    """åº”ç”¨å…³é—­æ—¶æ‰§è¡Œæ¸…ç†"""
    print("ğŸ›‘ åº”ç”¨å…³é—­ï¼šæ­£åœ¨åœæ­¢ MCP æœåŠ¡å™¨...")
    stop_mcp()
    print("âœ… åº”ç”¨å·²å…³é—­")


# å¯†ç è®¤è¯å›è°ƒå‡½æ•°
@cl.password_auth_callback
async def auth_callback(username: str, password: str):
    print(f">>> auth_callback user={username} pwd={password}")
    print(">>>", cl.User(identifier=username))

    return cl.User(identifier=username)
    # Fetch the user matching username from your database
    # and compare the hashed password with the value stored in the database
    # import hashlib
    # import json
    # from pathlib import Path

    # from sqlalchemy import text
    # from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
    # from sqlalchemy.orm import sessionmaker

    # # å¯†ç å“ˆå¸Œå‡½æ•°
    # def hash_password(raw_password):
    #     return hashlib.sha256(raw_password.encode("utf-8")).hexdigest()

    # # ç®€å•çš„é»˜è®¤ç”¨æˆ·éªŒè¯ï¼ˆç”¨äºå¼€å‘ç¯å¢ƒï¼‰
    # # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ç”¨æˆ·ï¼Œä½¿ç”¨é»˜è®¤éªŒè¯
    # if (username, password) == ("admin", "admin"):
    #     return cl.User(identifier="admin", metadata={"role": "admin", "provider": "credentials"})

    # # å°è¯•ä»æ•°æ®åº“éªŒè¯ç”¨æˆ·
    # db_path = Path(__file__).parent / "data" / "chat_sessions.db"
    # if not db_path.exists():
    #     # æ•°æ®åº“ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤éªŒè¯
    #     return None

    # try:
    #     db_url = f"sqlite+aiosqlite:///{db_path}"
    #     engine = create_async_engine(db_url, echo=False)
    #     async_session = sessionmaker(engine, expire_on_commit=False, class_=AsyncSession)

    #     async with async_session() as session:
    #         # æŸ¥è¯¢ç”¨æˆ·ï¼ˆusers è¡¨åªæœ‰ identifier, metadata, createdAt å­—æ®µï¼‰
    #         stmt = text("SELECT identifier, metadata FROM users WHERE identifier=:uname")
    #         result = await session.execute(stmt, {"uname": username})
    #         user_row = result.fetchone()

    #         if user_row is None:
    #             return None

    #         # ç”¨æˆ·å­˜åœ¨ï¼Œä» metadata ä¸­è·å–å¯†ç å“ˆå¸Œ
    #         db_username, db_metadata_raw = user_row

    #         # è§£æ metadataï¼ˆå¯èƒ½æ˜¯ JSON å­—ç¬¦ä¸²æˆ–å­—å…¸ï¼‰
    #         if isinstance(db_metadata_raw, str):
    #             try:
    #                 db_metadata = json.loads(db_metadata_raw)
    #             except json.JSONDecodeError:
    #                 db_metadata = {}
    #         else:
    #             db_metadata = db_metadata_raw or {}

    #         # ä» metadata ä¸­è·å–å¯†ç å“ˆå¸Œ
    #         db_password_hash = db_metadata.get("password_hash")

    #         # å¦‚æœ metadata ä¸­æ²¡æœ‰å¯†ç å“ˆå¸Œï¼Œè·³è¿‡å¯†ç éªŒè¯ï¼ˆä»…ç”¨äºå·²å­˜åœ¨çš„ç”¨æˆ·ï¼‰
    #         if db_password_hash is None:
    #             # å¦‚æœæ²¡æœ‰è®¾ç½®å¯†ç ï¼Œå…è®¸ç™»å½•ï¼ˆé¦–æ¬¡ç™»å½•ï¼‰
    #             return cl.User(identifier=db_username, metadata=db_metadata)

    #         # éªŒè¯å¯†ç 
    #         if db_password_hash == hash_password(password):
    #             return cl.User(identifier=db_username, metadata=db_metadata)
    #         else:
    #             return None
    # except Exception as e:
    #     print(f"æ•°æ®åº“éªŒè¯é”™è¯¯: {e}")
    #     # å¦‚æœæ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤éªŒè¯
    #     return None


# é…ç½®æ•°æ®æŒä¹…åŒ–
@cl.data_layer
def get_data_layer():
    # é…ç½® SQLite æ•°æ®åº“è¿æ¥
    data_path = Path(__file__).parent / "data"
    data_path.mkdir(exist_ok=True)  # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
    sqlite_conninfo = f"sqlite+aiosqlite:///./{data_path.name}/chat_sessions.db"
    sqlite_conninfo_with_params = f"{sqlite_conninfo}?cache=shared&journal_mode=WAL"

    # é…ç½®æœ¬åœ°æ–‡ä»¶å­˜å‚¨è·¯å¾„
    local_storage_path = f"./{data_path.name}/uploads"  # å­˜å‚¨ä¸Šä¼ æ–‡ä»¶çš„ç›®å½•
    # åˆ›å»ºæœ¬åœ°å­˜å‚¨å®¢æˆ·ç«¯
    storage_client = LocalStorageClient(local_storage_path)
    # åˆ›å»º SQLite æ•°æ®å±‚
    data_layer = SQLAlchemyDataLayer(conninfo=sqlite_conninfo_with_params, storage_provider=storage_client)

    return data_layer


# @cl.set_chat_profiles
# async def chat_profile(current_user: cl.User):
#     # if current_user.metadata["role"] != "ADMIN":
#     #     return None

#     return [
#         cl.ChatProfile(
#             name="My Chat Profile",
#             icon="https://picsum.photos/250",
#             markdown_description="The underlying LLM model is **GPT-3.5**, a *175B parameter model* trained on 410GB of text data.",
#             starters=[
#                 cl.Starter(
#                     label="Morning routine ideation",
#                     message="Can you help me create a personalized morning routine that would help increase my productivity throughout the day? Start by asking me about my current habits and what activities energize me in the morning.",
#                     icon="/public/idea.svg",
#                 ),
#                 cl.Starter(
#                     label="Explain superconductors",
#                     message="Explain superconductors like I'm five years old.",
#                     icon="/public/learn.svg",
#                 ),
#             ],
#         ),
#         cl.ChatProfile(
#             name="GPT-5",
#             markdown_description="The underlying LLM model is **GPT-5**.",
#             icon="https://picsum.photos/200",
#         ),
#     ]


@cl.set_starters
async def set_starters():
    return [
        cl.Starter(
            label="Morning routine ideation",
            message="Can you help me create a personalized morning routine that would help increase my productivity throughout the day? Start by asking me about my current habits and what activities energize me in the morning.",
            icon="/public/idea.svg",
        ),
        cl.Starter(
            label="Explain superconductors",
            message="Explain superconductors like I'm five years old.",
            icon="/public/learn.svg",
        ),
        cl.Starter(
            label="Python script for daily email reports",
            message="Write a script to automate sending daily email reports in Python, and walk me through how I would set it up.",
            icon="/public/terminal.svg",
            command="code",
        ),
        cl.Starter(
            label="Text inviting friend to wedding",
            message="Write a text asking a friend to be my plus-one at a wedding next month. I want to keep it super short and casual, and offer an out.",
            icon="/public/write.svg",
        ),
    ]


@cl.on_mcp_connect
async def on_mcp_connect(connection, session: ClientSession):
    """Called when an MCP connection is established"""
    print(">>> on_mcp_connect", connection.name)
    # List available tools
    result = await session.list_tools()

    # Process tool metadata
    tools = [
        {
            "name": t.name,
            "description": t.description,
            "input_schema": t.inputSchema,
        }
        for t in result.tools
    ]

    # Store tools for later use
    mcp_tools = cl.user_session.get("mcp_tools", {})
    mcp_tools[connection.name] = tools
    print(">>> mcp_tools", mcp_tools)
    cl.user_session.set("mcp_tools", mcp_tools)


@cl.on_mcp_disconnect
async def on_mcp_disconnect(name: str, session: ClientSession):
    """Called when an MCP connection is terminated"""
    # Your cleanup code here
    # This handler is optional
    print(">>> on_mcp_disconnect", name)


@cl.on_chat_start
async def chat_start():
    print(">>> on_chat_start, æ–°å»ºä¼šè¯ä¹Ÿä¼šè§¦å‘ï¼ï¼ï¼")

    # è®¾ç½®å‘½ä»¤
    await cl.context.emitter.set_commands(commands)

    cl.user_session.set("chat_history", [])

    settings = cl.user_session.get("settings", {})
    if not settings:
        settings = {
            "Model": "deepseek-r1:8b",
            "Think": True,
            "Temperature": 0.7,
            "Tags": ["Answer", "Chat-BI"],
        }
        cl.user_session.set("settings", settings)

    await cl.ChatSettings(
        [
            Select(
                id="Model",
                label="Select a model",
                values=[
                    "deepseek-r1:8b",
                    "deepseek-v3.1:671b-cloud",
                    "qwen2.5:1.5b",
                ],
                initial_value=settings["Model"],
            ),
            Switch(id="Think", label="Enable thinking for models", initial=settings["Think"]),
            Slider(
                id="Temperature",
                label="OpenAI - Temperature",
                initial=settings["Temperature"],
                min=0,
                max=1,
                step=0.1,
            ),
            Tags(id="Tags", label="OpenAI - StopSequence", initial=settings["Tags"]),
        ]
    ).send()

    # åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œå…³é”®æ˜¯æŒ‡å®š base_url ä¸º Ollama æœåŠ¡çš„åœ°å€
    client = AsyncOpenAI(
        base_url="http://localhost:11434/v1",  # Ollama çš„ OpenAI å…¼å®¹ API ç«¯ç‚¹
        api_key="unnecessary",  # æœ¬åœ°è¿è¡Œæ— éœ€æœ‰æ•ˆçš„ API å¯†é’¥ï¼Œå¯ä»»æ„å¡«å†™
    )
    # å°†å®¢æˆ·ç«¯å­˜å…¥ç”¨æˆ·ä¼šè¯ï¼Œæ–¹ä¾¿åç»­è°ƒç”¨

    # ############################################### Create the TaskList
    task_list = cl.TaskList()
    task_list.status = "Running..."

    # Create a task and put it in the running state
    task1 = cl.Task(title="Processing data", status=cl.TaskStatus.RUNNING)
    await task_list.add_task(task1)
    # Create another task that is in the ready state
    task2 = cl.Task(title="Performing calculations")
    await task_list.add_task(task2)

    # Optional: link a message to each task to allow task navigation in the chat history
    message = await cl.Message(content="Started processing data").send()
    task1.forId = message.id

    # Update the task list in the interface
    await task_list.send()

    # Perform some action on your end
    await cl.sleep(5)

    # Update the task statuses
    task1.status = cl.TaskStatus.DONE
    task2.status = cl.TaskStatus.FAILED
    task_list.status = "Failed"
    await task_list.send()
    # ###############################################
    await cl.ElementSidebar.set_title("ä»»åŠ¡é¢æ¿")
    await cl.ElementSidebar.set_elements(
        [],
        key="task-panel",
    )

    mcp_tools = cl.user_session.get("mcp_tools", [])
    print(">>> chat_start's mcp_tools", mcp_tools)

    llm = ChatOllama(
        # qwen2.5:1.5b, deepseek-v3.1:671b-cloud, deepseek-r1:8b
        model="deepseek-r1:8b",  # é€‰æ‹©ä»»ä½•ä½ æœ¬åœ°å·²æœ‰çš„æ¨¡å‹,deepseek-r1:8bä¸æ”¯æŒå·¥å…·è°ƒç”¨
        temperature=0.7,  # æ§åˆ¶ç”Ÿæˆç»“æœçš„éšæœºæ€§
        num_ctx=4096,  # è®¾ç½®ä¸Šä¸‹æ–‡çª—å£å¤§å°
    )

    agent = create_agent(
        model=llm,
        tools=[get_weather] + mcp_tools,
        system_prompt="You are a helpful assistant",
    )
    cl.user_session.set("agent", agent)

    # await cl.Message(content="ä½ å¥½ï¼æˆ‘å·²è¿æ¥æœ¬åœ°æ¨¡å‹ï¼Œè¯·å¼€å§‹æé—®ã€‚").send()


@cl.on_chat_resume
async def chat_resume(thread: ThreadDict):
    print(">>> on_chat_resume")

    # æ¢å¤è®¾ç½®
    settings = cl.user_session.get("settings")
    await cl.ChatSettings(
        [
            Select(
                id="Model",
                label="Select a model",
                values=[
                    "deepseek-r1:8b",
                    "deepseek-v3.1:671b-cloud",
                    "qwen2.5:1.5b",
                ],
                initial_value=settings["Model"],
            ),
            Switch(id="Think", label="Enable thinking for models", initial=settings["Think"]),
            Slider(
                id="Temperature",
                label="OpenAI - Temperature",
                initial=settings["Temperature"],
                min=0,
                max=1,
                step=0.1,
            ),
            Tags(id="Tags", label="OpenAI - StopSequence", initial=settings["Tags"]),
        ]
    ).send()

    # æ¢å¤èŠå¤©å†å²
    for message in thread.get("steps", []):
        if message.get("type") == "user_message":
            cl.user_session.get("chat_history").append(
                {
                    "role": "user",
                    "content": message.get("content"),
                }
            )
        elif message.get("type") == "assistant_message":
            cl.user_session.get("chat_history").append(
                {
                    "role": "assistant",
                    "content": message.get("content"),
                }
            )
        else:
            cl.user_session.get("chat_history").append(
                {
                    "role": "system",
                    "content": message.get("content"),
                }
            )


@cl.on_settings_update
async def settings_update(settings):
    # æ›´æ–°è®¾ç½®
    print(">>> on_settings_update", settings)
    cl.user_session.set("settings", settings)
    # TODO: æ›´æ–°å’Œå­˜å‚¨åˆ° cl.User.metadata ä¸­
    # cl.user_session.get("user").metadata = settings


@cl.on_message
async def main(message: cl.Message):
    try:
        # å¤„ç†å‘½ä»¤/æŒ‡ä»¤
        if message.command == "Database":
            # User is using the Picture command
            print(">>> Database command used")

        chat_history = cl.user_session.get("chat_history")
        chat_history.append(
            {
                "role": "user",
                "content": message.content,
            }
        )

        settings = cl.user_session.get("settings")
        model = settings["Model"]
        print(">>>> settings", settings)
        print(">>>> model", model)

        # qwen2.5:1.5b ä¸æ˜¯æ¨ç†æ¨¡å‹ï¼Œä¸æ”¯æŒ thinking
        settings["Think"] = False if settings["Think"] and model == "qwen2.5:1.5b" else settings["Think"]

        # ###########è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­#################
        # res = await cl.AskActionMessage(
        #     content="Pick an action!",
        #     actions=[
        #         cl.Action(name="continue", payload={"value": "continue"}, label="âœ… Continue"),
        #         cl.Action(name="cancel", payload={"value": "cancel"}, label="âŒ Cancel"),
        #     ],
        # ).send()

        # if res and res.get("payload").get("value") == "continue":
        #     await cl.Message(
        #         content="Continue!",
        #     ).send()
        # ###########################################

        llm = ChatOllama(
            # qwen2.5:1.5b, deepseek-v3.1:671b-cloud, deepseek-r1:8b
            model=model,  # é€‰æ‹©ä»»ä½•ä½ æœ¬åœ°å·²æœ‰çš„æ¨¡å‹,deepseek-r1:8bä¸æ”¯æŒå·¥å…·è°ƒç”¨
            temperature=settings["Temperature"],  # æ§åˆ¶ç”Ÿæˆç»“æœçš„éšæœºæ€§
            reasoning=settings["Think"],
            # num_ctx=4096,  # è®¾ç½®ä¸Šä¸‹æ–‡çª—å£å¤§å°
        )

        agent = create_agent(
            model=llm,
            tools=[get_weather] if model != "deepseek-r1:8b" else [],
            system_prompt="You are a helpful assistant",
        )

        stream = agent.stream(
            {"messages": chat_history},
            stream_mode="messages",
        )

        assistant_response = ""
        msg = cl.Message(content="")

        if settings["Think"]:
            start_time = time.time()
            async with cl.Step(name="Thinking", type="llm") as thinking_step:
                for token, metadata in stream:
                    reasoning = token.content_blocks[-1].get("reasoning", "") if token.content_blocks else ""
                    if reasoning:
                        # æµå¼ä¼ è¾“æ€è€ƒå†…å®¹
                        await thinking_step.stream_token(reasoning)
                        # thinking_step.output = reasoning

                    content = token.content_blocks[-1].get("text", "") if token.content_blocks else ""
                    if content:
                        # æ›´æ–°æ€è€ƒæ­¥éª¤åç§°å’Œæ—¶é—´
                        thought_duration = round(time.time() - start_time)
                        thinking_step.name = f"Thought for {thought_duration}s"
                        await thinking_step.update()

                        # æ›´æ–°é¦–ä¸ªcontent
                        assistant_response += content
                        await msg.stream_token(content)
                        # å…³é—­thinking_step
                        break

        # æµå¼ä¼ è¾“ assistant_response å†…å®¹
        for token, metadata in stream:
            content = token.content_blocks[-1].get("text", "") if token.content_blocks else ""
            if content:
                assistant_response += content
                await msg.stream_token(content)

        chat_history.append(
            {
                "role": "assistant",
                "content": assistant_response,
            }
        )
        await msg.update()

        # # åˆ¤æ–­æ˜¯å¦è°ƒç”¨å·¥å…·
        # # if chunk.choices[0].delta.tool_calls is not None:
        # tool_res = await tool()
        # await cl.Message(content=tool_res).send()

    except Exception:
        import traceback

        traceback.print_exc()
        await cl.Message(content=f"å‡ºç°å¼‚å¸¸: {traceback.format_exc()}").send()
        # await cl.Message(content=f"å‡ºç°å¼‚å¸¸: {e}").send()
